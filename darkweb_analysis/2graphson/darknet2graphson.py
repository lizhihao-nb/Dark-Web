graphson_dir = "./darknet_graphson/"

import pymongo
import json
import time
import os
import re

# è¿æ¥åˆ° MongoDB
client = pymongo.MongoClient("mongodb://192.168.31.9:27017/")
db = client['3our_spider_db']
collection = db["filtered_all"]

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(graphson_dir, exist_ok=True)

# æ´‹è‘±åœ°å€æ­£åˆ™ï¼ˆ56ä½ï¼‰
ONION_PATTERN = re.compile(r'([a-z0-9]{56})\.onion')

def extract_onion_address(url):
    """ä» URL ä¸­æå– 56 ä½æ´‹è‘±åœ°å€ï¼Œè‹¥æ— åˆ™è¿”å› None"""
    match = ONION_PATTERN.search(url)
    return match.group(1) if match else None

def add_inE(inE_dict, outV, edge, ID):
    inE_list = inE_dict.get("flow", [])
    tmp_dict = {"id": ID, "outV": outV, "properties": {"edge_kind": edge}}
    inE_list.append(tmp_dict)
    inE_dict["flow"] = inE_list
    return inE_dict

def add_outE(outE_dict, inV, edge, ID):
    outE_list = outE_dict.get("flow", [])
    tmp_dict = {"id": ID, "inV": inV, "properties": {"edge_kind": edge}}
    outE_list.append(tmp_dict)
    outE_dict["flow"] = outE_list
    return outE_dict

def add_properties(properties_dict, value, level):
    properties_dict.update(
        value=[{"id": "test_version", "value": value}],
        level=[{"id": "test_version", "value": level}]
    )
    return properties_dict

def darkweb_to_graphson(data_list, file_num):
    """å¤„ç†ä»»æ„é•¿åº¦çš„æ•°æ®åˆ—è¡¨ï¼ˆåŒ…æ‹¬å°‘äº BATCH_SIZE çš„æƒ…å†µï¼‰"""
    if not data_list:
        print("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ã€‚")
        return False

    file_links_dict = {}

    for link in data_list:
        id_str = str(link["_id"])
        url1 = link["url1"]
        url2 = link["url2"]
        edge_type = link['edge']

        level_value1 = "site" if (url1.count('/') == 2 and url1.count('.') == 1) else "page"
        level_value2 = "site" if (str(url2).count('/') == 2 and str(url2).count('.') == 1) else "page"

        # --- å¤„ç†å­èŠ‚ç‚¹ url2 ---
        if url2 not in ["-1", "1"]:
            link_url = url2
            if link_url in file_links_dict:
                node = file_links_dict[link_url]
                inE_dict = node.get("inE", {})
                node["inE"] = add_inE(inE_dict, url1, edge_type, id_str)
            else:
                props = add_properties({}, "", level_value2)
                if link["type2"] == "dark":
                    onion = extract_onion_address(link_url)
                    if onion:
                        props["site"] = [{"id": "onion_address", "value": onion}]
                node = {
                    "id": link_url,
                    "label": link["type2"],
                    "properties": props,
                    "inE": add_inE({}, url1, edge_type, id_str)
                }
                file_links_dict[link_url] = node

        # --- å¤„ç†çˆ¶èŠ‚ç‚¹ url1 ---
        if url1 != "NULL":
            link_url = url1
            if link_url in file_links_dict:
                node = file_links_dict[link_url]
                value_flag = "0" if url2 == "-1" else "1"
                node["properties"] = add_properties(node.get("properties", {}), value_flag, level_value1)
                if node["label"] == "dark" and "site" not in node["properties"]:
                    onion = extract_onion_address(link_url)
                    if onion:
                        node["properties"]["site"] = [{"id": "onion_address", "value": onion}]
                if url2 not in ["-1", "1"]:
                    outE_dict = node.get("outE", {})
                    node["outE"] = add_outE(outE_dict, url2, edge_type, id_str)
            else:
                value_flag = "0" if url2 == "-1" else "1"
                props = add_properties({}, value_flag, level_value1)
                if link["type1"] == "dark":
                    onion = extract_onion_address(link_url)
                    if onion:
                        props["site"] = [{"id": "onion_address", "value": onion}]
                node = {"id": link_url, "label": link["type1"], "properties": props}
                if url2 not in ["-1", "1"]:
                    node["outE"] = add_outE({}, url2, edge_type, id_str)
                file_links_dict[link_url] = node

    # å†™å…¥æ–‡ä»¶
    lines = [json.dumps(node, ensure_ascii=False) for node in file_links_dict.values()]
    current_json_file = f"Dark{file_num:05d}.json"
    with open(os.path.join(graphson_dir, current_json_file), 'w', encoding='utf-8') as fd:
        fd.write("\n".join(lines))

    print(f"âœ… æˆåŠŸå†™å…¥ {len(lines)} ä¸ªèŠ‚ç‚¹åˆ° {current_json_file}")
    return True

def monitor_and_process(batch_size=800000):
    """åªè¦æœ‰æœªå¤„ç†æ•°æ®ï¼ˆ>0ï¼‰ï¼Œå°±è¿”å›æœ€å¤š batch_size æ¡"""
    while True:
        count = collection.count_documents({"processed": {"$ne": True}})
        print(f"æœªå¤„ç†æ–‡æ¡£æ•°: {count}")
        if count > 0:
            documents = list(collection.find({"processed": {"$ne": True}}).limit(batch_size))
            return documents
        print("æš‚æ— æ•°æ®ï¼Œç­‰å¾…60ç§’...")
        time.sleep(60)

if __name__ == "__main__":
    count = 0
    BATCH_SIZE = 800000

    print("å¯åŠ¨ GraphSON å¯¼å‡ºæœåŠ¡...")
    while True:
        print("\nğŸ” æ£€æŸ¥æœªå¤„ç†æ•°æ®...")
        test_list = monitor_and_process(BATCH_SIZE)

        print(f"å¼€å§‹å¤„ç†ç¬¬ {count} æ‰¹ï¼Œå…± {len(test_list)} æ¡è®°å½•")

        success = darkweb_to_graphson(test_list, count)

        if success:
            ids_to_mark = [doc["_id"] for doc in test_list]
            result = collection.update_many(
                {"_id": {"$in": ids_to_mark}},
                {"$set": {"processed": True}}
            )
            print(f"ğŸ“Œ æ ‡è®° {result.modified_count} æ¡è®°å½•ä¸ºå·²å¤„ç†")
            print(f"âœ… No.{count} graphson OK")
            count += 1
        else:
            print("âŒ å†™å…¥å¤±è´¥ï¼è·³è¿‡æœ¬æ¬¡æ‰¹æ¬¡ï¼Œ5åˆ†é’Ÿåé‡è¯•...")
            time.sleep(300)

        print("å½“å‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚\n")