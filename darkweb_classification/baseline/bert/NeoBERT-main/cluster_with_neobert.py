# -*- coding: utf-8 -*-
import os
import torch
import numpy as np
import json
import random
from sklearn.cluster import KMeans
from openai import OpenAI
import pymongo
from bs4 import BeautifulSoup
import re
from transformers import AutoTokenizer, AutoModel

# é…ç½®
MONGO_HOST = "mongodb://192.168.31.9:27017/"
DB_NAME = "3our_spider_db"
COLLECTION_NAME = "content20000"
OPENAI_API_YOUR_KEY = "sk-cmz5LsPuRvfGFw9jhMa5Q89hoDVUoQYNaugjbX3zDIRDtIn6"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", OPENAI_API_YOUR_KEY)
OPENAI_BASE_URL = "https://api.chatanywhere.tech/v1"  # ç§»é™¤æœ«å°¾ç©ºæ ¼
N_CLUSTERS = 16
SAMPLES_PER_CLUSTER = 10
OUTPUT_PATH = "output/neobert_clustered_robust.json"
MODEL_NAME = "chandar-lab/NeoBERT"

def extract_clean_text(html_content: str) -> str:
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        for tag in soup(['script', 'style', 'meta', 'link', 'nav', 'footer', 'header']):
            tag.decompose()
        text = soup.get_text(separator=' ', strip=True)
        return re.sub(r'\s+', ' ', text).strip()
    except Exception:
        return re.sub(r'<[^>]+>', '', html_content)

def main():
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # ç§»é™¤æœ«å°¾ç©ºæ ¼

    # 1. è¯»å–æ•°æ®
    print("ğŸ“¥ è¯»å– MongoDB æ•°æ®...")
    client = pymongo.MongoClient(MONGO_HOST)
    collection = client[DB_NAME][COLLECTION_NAME]

    documents = []
    for doc in collection.find({"content": {"$exists": True}}, no_cursor_timeout=True):
        plain_text = extract_clean_text(doc["content"])
        if len(plain_text) < 50:
            continue

        summary = doc.get("summary", "").strip()
        documents.append({
            "text": plain_text,
            "summary": summary
        })

    print(f"âœ… åŠ è½½ {len(documents)} æ¡æœ‰æ•ˆæ–‡æœ¬")
    if len(documents) < N_CLUSTERS:
        print(f"âŒ æ–‡æœ¬æ•° ({len(documents)}) å°‘äºèšç±»æ•° ({N_CLUSTERS})ï¼Œé€€å‡º")
        return

    # 2. åŠ è½½ NeoBERT
    print("ğŸ”„ åŠ è½½ NeoBERT...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model.eval()

    # 3. æå–åµŒå…¥
    embeddings = []
    for doc in documents:
        inputs = tokenizer(doc["text"], return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
        embeddings.append(outputs.last_hidden_state[0, 0].cpu().numpy())
    embeddings = np.vstack(embeddings)

    # 4. èšç±»
    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42)
    labels = kmeans.fit_predict(embeddings)

    # 5. éšæœºæŠ½æ ·ï¼ˆæ¯ç°‡æœ€å¤š SAMPLES_PER_CLUSTER æ¡ï¼‰
    cluster_groups = {}
    for i, label in enumerate(labels):
        if label not in cluster_groups:
            cluster_groups[label] = []
        cluster_groups[label].append(documents[i]["text"])

    cluster_samples = {}
    for label, texts in cluster_groups.items():
        n_sample = min(SAMPLES_PER_CLUSTER, len(texts))
        sampled_texts = random.sample(texts, n_sample)
        cluster_samples[label] = sampled_texts
        print(f"èšç±» {label}: å…± {len(texts)} æ¡ â†’ æŠ½æ · {n_sample} æ¡")

    # 6. LLM ç›´æ¥ç”¨åŸå§‹æ–‡æœ¬ç”Ÿæˆç±»åˆ«æ ‡ç­¾ï¼ˆè·³è¿‡æ‘˜è¦ï¼‰
    llm_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
    cluster_categories = {}

    for cluster_id, texts in cluster_samples.items():
        # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼ˆæˆªæ–­åˆ° 300 å­—ç¬¦ï¼‰ï¼Œé¿å…è¶…é•¿
        prompt_texts = "\n".join(f"- {t[:300]}" for t in texts)
        prompt = f"""You are a text analysis expert. The following texts belong to the same semantic cluster. Please generate a concise category label consisting of 2 to 5 words.

Rules:
- Output ONLY the label, no explanation
- Use a noun phrase
- Avoid generic terms like "issue", "content", or "other"

Texts:
{prompt_texts}"""

        try:
            resp = llm_client.chat.completions.create(
                model="gpt-5-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                # max_tokens=100
            )

            # æ£€æŸ¥å“åº”ç»“æ„
            if not resp.choices or not resp.choices[0].message.content:
                print(f"âš ï¸ èšç±» {cluster_id}: LLM è¿”å›ç©ºå†…å®¹")
                label = ""
            else:
                label = resp.choices[0].message.content.strip()

            cluster_categories[cluster_id] = label if label else f"ç±»åˆ«_{cluster_id}"
            print(f"âœ… èšç±» {cluster_id} æ ‡ç­¾: '{label}'")

        except Exception as e:
            print(f"âŒ èšç±» {cluster_id} LLM è°ƒç”¨å¼‚å¸¸: {repr(e)}")
            import traceback
            traceback.print_exc()
            cluster_categories[cluster_id] = f"ç±»åˆ«_{cluster_id}"

    # 7. è¾“å‡ºæ‰€æœ‰æ–‡æ¡£ï¼ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„ summary ä½œä¸º alertï¼Œå¹¶è¿‡æ»¤æ— æ•ˆé¡¹ï¼‰
    output_data = []
    for i, doc in enumerate(documents):
        summary = doc.get("summary", "").strip()
        if not summary or summary == "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦":
            continue
        category = cluster_categories.get(labels[i], "æœªåˆ†ç±»")
        output_data.append({
            "alert": summary,
            "category": category
        })

    # 8. ä¿å­˜
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… å®Œæˆï¼ç»“æœä¿å­˜è‡³: {OUTPUT_PATH}")

if __name__ == "__main__":
    main()