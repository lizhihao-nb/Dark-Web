import json
import numpy as np
import re
import warnings
from bs4 import BeautifulSoup
import pymongo

# --- sklearn éƒ¨åˆ† ---
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

warnings.filterwarnings('ignore')

# ==================== 1. HTML æ–‡æœ¬æå– ====================
def extract_clean_text(html_content: str) -> str:
    """ä½¿ç”¨ BeautifulSoup ä» HTML æå–å¹²å‡€æ–‡æœ¬"""
    if not html_content or not isinstance(html_content, str):
        return ""
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        for tag in soup(['script', 'style', 'meta', 'link', 'nav', 'footer', 'header', 'aside']):
            tag.decompose()
        text = soup.get_text(separator=' ', strip=True)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    except Exception as e:
        return re.sub(r'<[^>]+>', ' ', html_content)

# ==================== 2. è‹±æ–‡æ–‡æœ¬é¢„å¤„ç† ====================
def simple_preprocess(text):
    text = re.sub(r'[^a-zA-Z\s]', ' ', text.lower())
    return ' '.join(text.split())

# ==================== 3. ä» MongoDB è¯»å– content20000 å…¨é‡æ•°æ® ====================
print("ğŸ“¤ æ­£åœ¨ä» MongoDB è¯»å– content20000 é›†åˆä¸­çš„å…¨éƒ¨ HTML å†…å®¹...")
client = pymongo.MongoClient("mongodb://192.168.31.9:27017/")
db = client["3our_spider_db"]
collection = db["content20000"]

original_alerts = []
processed_texts = []

cursor = collection.find(
    {"content": {"$exists": True, "$type": "string", "$ne": ""}},
    {"content": 1}
)

count = 0
for doc in cursor:
    try:
        html = doc["content"]
        clean_text = extract_clean_text(html)
        
        if len(clean_text) < 20:
            continue
            
        original_alerts.append(clean_text)
        en_only = simple_preprocess(clean_text)
        processed_texts.append(en_only if en_only else clean_text.lower())
        
        count += 1
        if count % 1000 == 0:
            print(f"  å·²å¤„ç† {count} æ¡...")
    except Exception as e:
        continue

# å¯¹é½é•¿åº¦ï¼ˆå®‰å…¨èµ·è§ï¼‰
min_len = min(len(original_alerts), len(processed_texts))
original_alerts = original_alerts[:min_len]
processed_texts = processed_texts[:min_len]

print(f"âœ… æˆåŠŸåŠ è½½å¹¶æ¸…æ´— {len(original_alerts)} æ¡æœ‰æ•ˆæ–‡æœ¬")

if not original_alerts:
    raise ValueError("æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬å¯ç”¨äºèšç±»ï¼")

# ==================== 4. K-means èšç±»é€»è¾‘ ====================

def find_optimal_clusters_kmeans_fast(tfidf_matrix, max_clusters=20):
    """å¿«é€Ÿè®¾å®šèšç±»æ•°ï¼ˆå¤§æ•°æ®è·³è¿‡å¤æ‚è¯„ä¼°ï¼‰"""
    max_k = min(max_clusters + 1, tfidf_matrix.shape[0])
    if max_k <= 2:
        return 2
    chosen = min(20, max_k - 1)  # é»˜è®¤åˆç†å€¼
    print(f"ğŸ’¡ K-means èšç±»æ•°è®¾ä¸º {chosen}")
    return chosen

# ========== å‡†å¤‡ TF-IDF å‘é‡è¡¨ç¤º ==========
print("\nğŸ§  æ„å»º TF-IDF å‘é‡ï¼ˆmax_features=100ï¼‰...")
vectorizer = TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(1, 2))
tfidf_matrix = vectorizer.fit_transform(processed_texts)

# ========== è‡ªåŠ¨é€‰æ‹©èšç±»æ•° ==========
optimal_kmeans = find_optimal_clusters_kmeans_fast(tfidf_matrix, max_clusters=50)
print(f"\nğŸ“Œ K-means èšç±»æ•°: {optimal_kmeans}\n")

# ========== æ‰§è¡Œ K-means ==========
print("ğŸ§© è¿è¡Œ K-means èšç±»...")
kmeans = KMeans(n_clusters=optimal_kmeans, random_state=42, n_init=5, max_iter=50)
kmeans_labels = kmeans.fit_predict(tfidf_matrix)

# ========== ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆæ ‡ç­¾ï¼ˆåŸºäº top-3 TF-IDF è¯ï¼‰==========
feature_names = vectorizer.get_feature_names()
kmeans_cluster_labels = {}
for i in range(optimal_kmeans):
    mask = (kmeans_labels == i)
    if np.sum(mask) == 0:
        label = "empty"
    else:
        avg_tfidf = np.array(tfidf_matrix[mask].mean(axis=0)).flatten()
        top_indices = avg_tfidf.argsort()[-3:][::-1]
        top_words = [feature_names[idx] for idx in top_indices]
        label = '_'.join(top_words)
    kmeans_cluster_labels[i] = label
    print(f"K-means èšç±» {i}: {label}")

# ========== ä¿å­˜ç»“æœ ==========
kmeans_results = [
    {"alert": text, "category": kmeans_cluster_labels[label]}
    for text, label in zip(original_alerts, kmeans_labels)
]

with open('tfidf_results.json', 'w', encoding='utf-8') as f:
    json.dump(kmeans_results, f, ensure_ascii=False, indent=2)
print(f"âœ… å·²ä¿å­˜ K-means ç»“æœåˆ° tfidf_results.json ({len(kmeans_results)} æ¡)")

# ========== ç»Ÿè®¡åˆ†å¸ƒ ==========
from collections import Counter
print("\n" + "="*50)
print("ğŸ“Š K-means èšç±»åˆ†å¸ƒ")
print("="*50)
kmeans_dist = Counter(kmeans_labels)
for cid, cnt in kmeans_dist.most_common():
    print(f"  {kmeans_cluster_labels[cid]}: {cnt} æ¡")

print("\nğŸ‰ K-means èšç±»å®Œæˆï¼ç»“æœå·²ä¿å­˜ã€‚")