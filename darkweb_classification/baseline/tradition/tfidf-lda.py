import json
import numpy as np
import re
import warnings
from bs4 import BeautifulSoup
import pymongo

# --- sklearn éƒ¨åˆ† ---
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.cluster import KMeans

warnings.filterwarnings('ignore')

# ==================== 1. HTML æ–‡æœ¬æå– ====================
def extract_clean_text(html_content: str) -> str:
    """ä½¿ç”¨ BeautifulSoup ä» HTML æå–å¹²å‡€æ–‡æœ¬"""
    if not html_content or not isinstance(html_content, str):
        return ""
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        for tag in soup(['script', 'style', 'meta', 'link', 'nav', 'footer', 'header', 'aside']):
            tag.decompose()
        text = soup.get_text(separator=' ', strip=True)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    except Exception as e:
        return re.sub(r'<[^>]+>', ' ', html_content)

# ==================== 2. è‹±æ–‡æ–‡æœ¬é¢„å¤„ç† ====================
def simple_preprocess(text):
    text = re.sub(r'[^a-zA-Z\s]', ' ', text.lower())
    return ' '.join(text.split())

# ==================== 3. ä» MongoDB è¯»å– content20000 å…¨é‡æ•°æ® ====================
print("ğŸ“¤ æ­£åœ¨ä» MongoDB è¯»å– content20000 é›†åˆä¸­çš„å…¨éƒ¨ HTML å†…å®¹...")
client = pymongo.MongoClient("mongodb://192.168.31.9:27017/")
db = client["3our_spider_db"]
collection = db["content20000"]  # â† ä¿®æ”¹ä¸ºä½ çš„ç›®æ ‡é›†åˆ

original_alerts = []
processed_texts = []

cursor = collection.find(
    {"content": {"$exists": True, "$type": "string", "$ne": ""}},
    {"content": 1}
)

count = 0
for doc in cursor:
    try:
        html = doc["content"]
        clean_text = extract_clean_text(html)
        
        if len(clean_text) < 20:
            continue
            
        original_alerts.append(clean_text)
        en_only = simple_preprocess(clean_text)
        processed_texts.append(en_only if en_only else clean_text.lower())
        
        count += 1
        if count % 1000 == 0:
            print(f"  å·²å¤„ç† {count} æ¡...")
    except Exception as e:
        continue  # è·³è¿‡å¼‚å¸¸æ–‡æ¡£

# å¯¹é½é•¿åº¦
min_len = min(len(original_alerts), len(processed_texts))
original_alerts = original_alerts[:min_len]
processed_texts = processed_texts[:min_len]

print(f"âœ… æˆåŠŸåŠ è½½å¹¶æ¸…æ´— {len(original_alerts)} æ¡æœ‰æ•ˆæ–‡æœ¬")

if not original_alerts:
    raise ValueError("æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬å¯ç”¨äºèšç±»ï¼")

# ==================== 4. èšç±»é€»è¾‘ ====================

def find_optimal_topics_lda(doc_term_matrix, max_topics=50):
    max_k = min(max_topics + 1, doc_term_matrix.shape[0])
    if max_k <= 2:
        return 2, []
    topic_range = range(5, max_k, 5)  # ä»5å¼€å§‹ï¼Œæ­¥é•¿5ï¼Œå‡å°‘è®¡ç®—
    if 2 not in topic_range:
        topic_range = [2] + list(topic_range)
    topic_range = sorted(set(topic_range))
    
    perplexities = []
    print("ğŸ” å¯»æ‰¾æœ€ä¼˜ LDA ä¸»é¢˜æ•°ï¼ˆåŸºäºå›°æƒ‘åº¦ï¼‰...")
    best_n, best_ppl = 2, float('inf')
    for n in topic_range:
        if n >= doc_term_matrix.shape[0]:
            break
        lda = LatentDirichletAllocation(n_components=n, random_state=42, max_iter=50)
        lda.fit(doc_term_matrix)
        ppl = lda.perplexity(doc_term_matrix)
        perplexities.append(ppl)
        print(f"  Topics: {n}, Perplexity: {ppl:.2f}")
        if ppl < best_ppl:
            best_ppl = ppl
            best_n = n
    return best_n, perplexities

def find_optimal_clusters_kmeans_fast(tfidf_matrix, max_clusters=50):
    """ä½¿ç”¨ inertiaï¼ˆè‚˜éƒ¨æ³•ï¼‰å¿«é€Ÿä¼°è®¡åˆç†èšç±»æ•°ï¼Œé¿å… silhouette"""
    max_k = min(max_clusters + 1, tfidf_matrix.shape[0])
    if max_k <= 2:
        return 2
    # å¯¹äºå¤§æ•°æ®ï¼Œé»˜è®¤é€‰æ‹©ä¸€ä¸ªåˆç†å€¼ï¼ˆå¦‚30ï¼‰ï¼Œæˆ–æ ¹æ®ç»éªŒè°ƒæ•´
    # è¿™é‡Œæˆ‘ä»¬ç®€å•è¿”å› min(30, max_k-1)
    chosen = min(30, max_k - 1)
    print(f"ğŸ’¡ K-means èšç±»æ•°è®¾ä¸º {chosen}ï¼ˆå¤§æ•°æ®è·³è¿‡ silhouette è®¡ç®—ï¼‰")
    return chosen

# ========== å‡†å¤‡å‘é‡è¡¨ç¤º ==========
print("\nğŸ§  å‡†å¤‡å‘é‡è¡¨ç¤ºï¼ˆmax_features=1000ï¼‰...")
vectorizer_lda = CountVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))
doc_term_matrix_lda = vectorizer_lda.fit_transform(processed_texts)

vectorizer_kmeans = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))
tfidf_matrix = vectorizer_kmeans.fit_transform(processed_texts)

# ========== è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•° ==========
optimal_lda, _ = find_optimal_topics_lda(doc_term_matrix_lda, max_topics=50)
optimal_kmeans = find_optimal_clusters_kmeans_fast(tfidf_matrix, max_clusters=50)

print(f"\nğŸ“Œ æœ€ä¼˜ LDA ä¸»é¢˜æ•°: {optimal_lda}")
print(f"ğŸ“Œ K-means èšç±»æ•°: {optimal_kmeans}\n")

# ========== LDA ==========
print("ğŸ§© è¿è¡Œ LDA èšç±»...")
lda = LatentDirichletAllocation(n_components=optimal_lda, random_state=42, max_iter=100)
lda_doc_topic = lda.fit_transform(doc_term_matrix_lda)
lda_topics = np.argmax(lda_doc_topic, axis=1)

feature_names_lda = vectorizer_lda.get_feature_names()
lda_labels = {}
for i in range(optimal_lda):
    top_words = [feature_names_lda[idx] for idx in lda.components_[i].argsort()[-3:][::-1]]
    lda_labels[i] = '_'.join(top_words)
    print(f"LDA ä¸»é¢˜ {i}: {lda_labels[i]}")

lda_results = [
    {"alert": text, "category": lda_labels[topic]}
    for text, topic in zip(original_alerts, lda_topics)
]

with open('lda_results.json', 'w', encoding='utf-8') as f:
    json.dump(lda_results, f, ensure_ascii=False, indent=2)
print(f"âœ… å·²ä¿å­˜ LDA ç»“æœåˆ° lda_results.json ({len(lda_results)} æ¡)")

# ========== K-means ==========
print("\nğŸ§© è¿è¡Œ K-means èšç±»...")
kmeans = KMeans(n_clusters=optimal_kmeans, random_state=42, n_init=5, max_iter=100)
kmeans_labels = kmeans.fit_predict(tfidf_matrix)

feature_names_km = vectorizer_kmeans.get_feature_names()
kmeans_cluster_labels = {}
for i in range(optimal_kmeans):
    mask = (kmeans_labels == i)
    if np.sum(mask) == 0:
        label = "empty"
    else:
        avg_tfidf = np.array(tfidf_matrix[mask].mean(axis=0)).flatten()
        top_words = [feature_names_km[idx] for idx in avg_tfidf.argsort()[-3:][::-1]]
        label = '_'.join(top_words)
    kmeans_cluster_labels[i] = label
    print(f"K-means èšç±» {i}: {label}")

kmeans_results = [
    {"alert": text, "category": kmeans_cluster_labels[label]}
    for text, label in zip(original_alerts, kmeans_labels)
]

with open('tfidf_results.json', 'w', encoding='utf-8') as f:
    json.dump(kmeans_results, f, ensure_ascii=False, indent=2)
print(f"âœ… å·²ä¿å­˜ K-means ç»“æœåˆ° tfidf_results.json ({len(kmeans_results)} æ¡)")

# ========== ç»Ÿè®¡ ==========
from collections import Counter
print("\n" + "="*50)
print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
print("="*50)
print(f"LDA ä½¿ç”¨ {optimal_lda} ä¸ªä¸»é¢˜")
print(f"K-means ä½¿ç”¨ {optimal_kmeans} ä¸ªèšç±»")

lda_dist = Counter(lda_topics)
print("\nLDA ä¸»é¢˜åˆ†å¸ƒ:")
for tid, cnt in lda_dist.most_common():
    print(f"  {lda_labels[tid]}: {cnt} æ¡")

kmeans_dist = Counter(kmeans_labels)
print("\nK-means èšç±»åˆ†å¸ƒ:")
for cid, cnt in kmeans_dist.most_common():
    print(f"  {kmeans_cluster_labels[cid]}: {cnt} æ¡")

print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç»“æœå·²ä¿å­˜ä¸º *_20k.json æ–‡ä»¶ã€‚")