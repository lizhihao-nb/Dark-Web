

# è¿™æ˜¯label accuracyçš„é¢˜ç›®æ•°é‡
LABEL_SAMPLE_SIZE = 1000
# è¿™æ˜¯word intrusionï¼Œæ¯ä¸ªç±»åˆ«çš„é¢˜ç›®æ•°é‡
INTRUSION_ROUNDS = 5
# è¿™æ˜¯ä¿å­˜ç»“æœåœ¨å“ªä¸ªæ–‡ä»¶
OUTPUT_FILE = 'lightML_evaluation_results.json'

import json
import argparse
import random
import requests
from collections import defaultdict, Counter
import re
from typing import List, Dict, Tuple, Optional
import time
import math

# --- é…ç½® ---
API_URL = "https://api.chatanywhere.tech/v1/chat/completions"
API_KEY = "sk-cmz5LsPuRvfGFw9jhMa5Q89hoDVUoQYNaugjbX3zDIRDtIn6"
MODEL_NAME = "gpt-5-mini"  # æˆ–ä½ å®é™…å¯ç”¨çš„æ¨¡å‹
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}
random.seed(42)
EXPECTED_MAX_CATEGORIES = 100  # ä½ è®¤ä¸ºåˆç†çš„æœ€å¤§ç±»åˆ«æ•°ï¼Œå¯è°ƒ

# --- Promptæ¨¡æ¿ï¼ˆå·²é€‚é…æš—ç½‘åœºæ™¯ï¼‰---
LABEL_ACCURACY_PROMPT = """You are analyzing dark web content. Please select the most appropriate category for the following text.

Text: {alert}

Available categories:
1. {positive_category}
2. {negative_category}

Please **ONLY answer with number 1 or 2**."""

WORD_INTRUSION_PROMPT = """You are analyzing dark web content. Five texts belong to the same category, and one is from a different category (the intrusion).
Identify the intrusion by its number (1-6).

Texts:
{sample_list}

Please **ONLY answer with a single number (1-6)**."""

# --- è¯„ä¼°ç»“æœå­˜å‚¨ ---
evaluation_results = {
    # 'regex_accuracy': 0.0,
    'label_accuracy': 0.0,
    # 'label_accuracy_adjusted': 0.0,
    'label_accuracy_details': [],
    'word_intrusion_accuracy': 0.0,
    # 'word_intrusion_adjusted': 0.0,
    'word_intrusion_details': [],
    'actual_category_count': 0,
    # 'category_penalty_factor': 1.0
}

def call_llm(messages, system_prompt="You are an expert in dark web content analysis and classification.", temperature=0.1):
    """è°ƒç”¨ LLM API çš„é€šç”¨å‡½æ•°ï¼ˆé€‚é…æš—ç½‘ä»»åŠ¡ï¼‰"""
    time.sleep(3)  # é˜²æ­¢ API é™æµ
    payload = {
        "model": MODEL_NAME,
        "messages": messages,
    }
    if system_prompt:
        payload["messages"].insert(0, {"role": "system", "content": system_prompt})

    try:
        response = requests.post(API_URL, json=payload, headers=HEADERS)
        response.raise_for_status()
        result = response.json()
        print(f"LLM Response: {result}")  # è°ƒè¯•ç”¨
        return result['choices'][0]['message']['content']
    except requests.exceptions.RequestException as e:
        print(f"[Retrying] Error calling LLM API: {e}")
        if hasattr(e, 'response') and e.response is not None:
            print(f"Response content: {e.response.text}")
        return call_llm(messages)
    except (KeyError, IndexError) as e:
        print(f"[Retrying] Error parsing LLM response: {e}")
        print(f"Response content: {response.text if 'response' in locals() else 'No response'}")
        return call_llm(messages)

def load_data(file_path):
    """åŠ è½½JSONæ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

# def regex_based_evaluation(data):
#     """åŸºäºæ­£åˆ™çš„è¯„ä¼°ï¼šåˆ¤æ–­categoryæ˜¯å¦å‡ºç°åœ¨textä¸­ï¼ˆæ— è§†å¤§å°å†™ï¼‰"""
#     correct_count = 0
#     total_count = len(data)
    
#     for item in data:
#         text = item.get('alert', '')  # ä¿ç•™å­—æ®µåä¸º 'alert' ä»¥å…¼å®¹è¾“å…¥
#         category = item.get('category', '')
        
#         if not category:
#             continue
            
#         if re.search(re.escape(category), text, re.IGNORECASE):
#             correct_count += 1
#         else:
#             text_clean = re.sub(r'\s+', '', text.lower())
#             category_clean = re.sub(r'\s+', '', category.lower())
#             if category_clean in text_clean:
#                 correct_count += 1
    
#     accuracy = correct_count / total_count if total_count > 0 else 0
#     evaluation_results['regex_accuracy'] = accuracy
#     print(f"åŸºäºæ­£åˆ™çš„å‡†ç¡®ç‡ï¼ˆæš—ç½‘æ–‡æœ¬ï¼‰: {accuracy:.4f} ({correct_count}/{total_count})")
#     return accuracy

def label_accuracy_evaluation(data, sample_size):
    """Label Accuracyè¯„ä¼°ï¼šä½¿ç”¨LLMåˆ¤æ–­æš—ç½‘åˆ†ç±»æ˜¯å¦åˆç†"""
    all_categories = list(set(item.get('category', '') for item in data if item.get('category')))
    actual_category_count = len(all_categories)
    if actual_category_count < 2:
        print("ç±»åˆ«æ•°å°‘äº2ï¼Œæ— æ³•è¿›è¡ŒLabel Accuracyè¯„ä¼°")
        return 0.0

    sample_data = random.sample(data, min(sample_size, len(data)))
    correct_count = 0
    total_processed = 0
    details = []

    for i, item in enumerate(sample_data):
        original_category = item.get('category', '')
        text = item.get('alert', '')
        if not original_category or not text:
            continue

        negative_categories = [cat for cat in all_categories if cat != original_category]
        if not negative_categories:
            continue
        negative_category = random.choice(negative_categories)

        prompt = LABEL_ACCURACY_PROMPT.format(
            alert=text,
            positive_category=original_category,
            negative_category=negative_category
        )
        messages = [{"role": "user", "content": prompt}]
        response = call_llm(messages)

        detail = {
            'index': i,
            'text_snippet': text[:100] + '...' if len(text) > 100 else text,
            'positive_category': original_category,
            'negative_category': negative_category,
            'llm_response': response,
            'is_correct': False
        }

        # TODO: è¿™é‡Œå…¶å®åˆ¤æ–­æ¯”è¾ƒè‰ç‡
        if response:
            response = response.strip()
            if '1' in response:
                correct_count += 1
                detail['is_correct'] = True
            elif '2' in response:
                detail['is_correct'] = False
            total_processed += 1

        details.append(detail)

    raw_accuracy = correct_count / total_processed if total_processed > 0 else 0
    # penalty_factor = max(1.0, math.log(actual_category_count) / math.log(EXPECTED_MAX_CATEGORIES))
    # adjusted_accuracy = raw_accuracy / penalty_factor

    evaluation_results['label_accuracy'] = raw_accuracy
    # evaluation_results['label_accuracy_adjusted'] = adjusted_accuracy
    evaluation_results['label_accuracy_details'] = details
    evaluation_results['actual_category_count'] = actual_category_count
    # evaluation_results['category_penalty_factor'] = penalty_factor

    print(f"Label Accuracy (åŸå§‹): {raw_accuracy:.4f} ({correct_count}/{total_processed})")
    # print(f"Label Accuracy (è°ƒæ•´å, {actual_category_count}ç±» â†’ penalty={penalty_factor:.2f}): {adjusted_accuracy:.4f}")
    return raw_accuracy

def word_intrusion_evaluation(data, rounds_per_category=5):
    """Word Intrusionè¯„ä¼°ï¼šæ£€æµ‹æš—ç½‘ç±»åˆ«å†…éƒ¨ä¸€è‡´æ€§"""
    category_to_items = defaultdict(list)
    for item in data:
        cat = item.get('category', '')
        txt = item.get('alert', '')
        if cat and txt:
            category_to_items[cat].append(item)

    valid_categories = {cat: items for cat, items in category_to_items.items() if len(items) >= 5}
    if len(valid_categories) < 2:
        print("æœ‰æ•ˆç±»åˆ«æ•°å°‘äº2ï¼Œæ— æ³•è¿›è¡ŒWord Intrusionè¯„ä¼°")
        return 0.0

    detected_count = 0
    total_rounds = 0
    details = []

    for category, positive_items in valid_categories.items():
        other_categories = [c for c in valid_categories if c != category]
        for _ in range(rounds_per_category):
            if len(positive_items) < 5 or not other_categories:
                continue

            selected_positive = random.sample(positive_items, 5)
            neg_cat = random.choice(other_categories)
            neg_item = random.choice(valid_categories[neg_cat])

            all_samples = selected_positive + [neg_item]
            random.shuffle(all_samples)

            sample_texts = [f"{i+1}. {s.get('alert', '')}" for i, s in enumerate(all_samples)]
            sample_list_str = '\n'.join(sample_texts)
            prompt = WORD_INTRUSION_PROMPT.format(sample_list=sample_list_str)

            messages = [{"role": "user", "content": prompt}]
            response = call_llm(messages)

            detail = {
                'category': category,
                'negative_category': neg_cat,
                'llm_response': response,
                'is_detected': False
            }

            try:
                shuffled_index = all_samples.index(neg_item)
                expected_answer = str(shuffled_index + 1)
                detail['expected_answer'] = expected_answer
                detail['actual_answer'] = response.strip() if response else ""

                if response and expected_answer in response:
                    detected_count += 1
                    detail['is_detected'] = True
                total_rounds += 1
            except ValueError:
                detail['error'] = 'Negative sample not found in shuffled list'

            details.append(detail)

    actual_category_count = len(category_to_items)
    evaluation_results['actual_category_count'] = actual_category_count
    # # å¤ç”¨æˆ–è®¡ç®— penalty factor
    # if 'category_penalty_factor' not in evaluation_results:
    #     actual_category_count = len(category_to_items)
    #     penalty_factor = max(1.0, math.log(actual_category_count) / math.log(EXPECTED_MAX_CATEGORIES))
    #     evaluation_results['actual_category_count'] = actual_category_count
    #     evaluation_results['category_penalty_factor'] = penalty_factor
    # else:
    #     penalty_factor = evaluation_results['category_penalty_factor']

    raw_accuracy = detected_count / total_rounds if total_rounds > 0 else 0
    # adjusted_accuracy = raw_accuracy / penalty_factor

    evaluation_results['word_intrusion_accuracy'] = raw_accuracy
    # evaluation_results['word_intrusion_adjusted'] = adjusted_accuracy
    evaluation_results['word_intrusion_details'] = details

    print(f"Word Intrusion Accuracy (åŸå§‹): {raw_accuracy:.4f} ({detected_count}/{total_rounds})")
    # print(f"Word Intrusion Accuracy (è°ƒæ•´å, penalty={penalty_factor:.2f}): {adjusted_accuracy:.4f}")
    return raw_accuracy

def save_results(output_file):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
    print(f"æš—ç½‘åˆ†ç±»è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°æš—ç½‘æ–‡æœ¬åˆ†ç±»æ•ˆæœ')
    parser.add_argument('input_file', help='è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆæ¯æ¡å« "alert" æ–‡æœ¬å’Œ "category" æ ‡ç­¾ï¼‰')
    parser.add_argument('--output-file', default=OUTPUT_FILE, help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--label-sample-size', type=int, default=LABEL_SAMPLE_SIZE, help='Label Accuracyè¯„ä¼°çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--intrusion-rounds', type=int, default=INTRUSION_ROUNDS, help='æ¯ä¸ªç±»åˆ«çš„Word Intrusionè¯„ä¼°è½®æ•°')
    
    args = parser.parse_args()
    
    print("æ­£åœ¨åŠ è½½æš—ç½‘æ–‡æœ¬æ•°æ®...")
    data = load_data(args.input_file)
    print(f"å…±åŠ è½½ {len(data)} æ¡æš—ç½‘æ–‡æœ¬")

    # print("\n=== åŸºäºæ­£åˆ™çš„è¯„ä¼°ï¼ˆBaselineï¼‰===")
    # regex_acc = regex_based_evaluation(data)
    
    print("\n=== Label Accuracy è¯„ä¼°ï¼ˆLLM åˆ¤æ–­åˆ†ç±»åˆç†æ€§ï¼‰===")
    label_acc = label_accuracy_evaluation(data, args.label_sample_size)
    
    print("\n=== Word Intrusion è¯„ä¼°ï¼ˆLLM æ£€æµ‹ç±»åˆ«å†…èšæ€§ï¼‰===")
    intrusion_acc = word_intrusion_evaluation(data, args.intrusion_rounds)
    
    print("\n=== æš—ç½‘åˆ†ç±»è¯„ä¼°æ±‡æ€» ===")
    # print(f"æ­£åˆ™å‡†ç¡®ç‡:       {regex_acc:.4f}")
    print(f"Label Accuracy:   {label_acc:.4f}")
    print(f"Word Intrusion:   {intrusion_acc:.4f}")
    
    save_results(args.output_file)

    # # ç±»åˆ«å¥åº·åº¦æç¤º
    # cat_count = evaluation_results.get('actual_category_count', 0)
    # penalty = evaluation_results.get('category_penalty_factor', 1.0)
    # print(f"\nğŸ“Š åˆ†ç±»ä½“ç³»å¥åº·åº¦æç¤º:")
    # print(f"   æ€»ç±»åˆ«æ•°: {cat_count}")
    # print(f"   æƒ©ç½šå› å­: {penalty:.2f}")
    # if cat_count > 2 * EXPECTED_MAX_CATEGORIES:
    #     print(f"   âš ï¸  è­¦å‘Šï¼šç±»åˆ«ä¸¥é‡è¿‡ç»†ï¼ˆ>{2*EXPECTED_MAX_CATEGORIES}ï¼‰ï¼å»ºè®®åˆå¹¶ç›¸ä¼¼ç±»ã€‚")
    # elif cat_count > EXPECTED_MAX_CATEGORIES:
    #     print(f"   â„¹ï¸  æç¤ºï¼šç±»åˆ«æ•°åå¤šï¼ˆ>{EXPECTED_MAX_CATEGORIES}ï¼‰ï¼Œå¯èƒ½å½±å“æ³›åŒ–ã€‚")
    # else:
    #     print(f"   âœ… ç±»åˆ«æ•°é‡åœ¨åˆç†èŒƒå›´å†…ã€‚")

if __name__ == "__main__":
    main()