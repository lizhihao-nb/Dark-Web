import os
import pymongo
import re
import time
import json
import numpy as np
from bs4 import BeautifulSoup
from openai import OpenAI
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Tuple
from scipy.spatial.distance import cosine
from bson import ObjectId

# å¯¼å…¥æœºå™¨å­¦ä¹ ç›¸å…³åº“
import pickle
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder


class TaxonomyBuilder:
    
    def __init__(self):
        # åˆå§‹åŒ–é…ç½®
        self.config = {
            "mongo_host": "mongodb://192.168.31.9:27017/",
            "db_name": "3our_spider_db",
            "collection_name": "content20000",
            "openai_api_key": os.getenv("OPENAI_API_KEY", ""),
            "openai_base_url": "https://api.chatanywhere.tech/v1    ",
            "max_summary_length": 100,
            "max_text_length": 8000,
            "batch_size": 40,
            "request_delay": 1,
            "taxonomy_batch_size": 500,
            "max_taxonomy_updates": 10,
            "distance_threshold": 0.8,
            "min_samples_for_centroid": 3,
        }
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = None
        self.db = None
        self.collection = None
        self.openai_client = None
        
        # ä¸»é¢˜ç®¡ç†
        self.all_summaries = []
        self.final_taxonomy = []  # List[{"category": "..."}]
        
        # å‘é‡ç®¡ç†
        self.category_centroids = {}
        self.category_vectors = defaultdict(list)
        self.document_vectors = {}
        
        # æœºå™¨å­¦ä¹ æ¨¡å‹
        self.ml_model = None
        self.label_encoder = None
        
        self._init_clients()
    
    def _init_clients(self):
        """åˆå§‹åŒ–æ•°æ®åº“å’ŒOpenAIå®¢æˆ·ç«¯"""
        try:
            self.client = pymongo.MongoClient(self.config["mongo_host"])
            self.db = self.client[self.config["db_name"]]
            self.collection = self.db[self.config["collection_name"]]
            print("MongoDBè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"MongoDBè¿æ¥å¤±è´¥: {e}")
            raise
        
        try:
            self.openai_client = OpenAI(
                api_key=self.config["openai_api_key"],
                base_url=self.config["openai_base_url"].strip()
            )
            print("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _migrate_category_vectors(self, merged_from: Dict[str, List[str]]):
        """
        æ ¹æ® merged_from æ˜ å°„ï¼Œå°†æ—§ç±»åˆ«çš„å‘é‡è¿ç§»åˆ°æ–°ç±»åˆ«ï¼Œå¹¶æ¸…ç†æ—§é”®ã€‚
        """
        new_vectors = defaultdict(list)
        
        # åˆå§‹åŒ–æ–°ç±»åˆ«å®¹å™¨ï¼ˆä¿ç•™å¯èƒ½å·²å­˜åœ¨çš„å‘é‡ï¼‰
        for new_cat in merged_from.keys():
            if new_cat in self.category_vectors:
                new_vectors[new_cat] = self.category_vectors[new_cat].copy()
            else:
                new_vectors[new_cat] = []
        
        # è¿ç§»æ—§å‘é‡
        for new_cat, old_list in merged_from.items():
            for old_cat in old_list:
                if old_cat in self.category_vectors:
                    new_vectors[new_cat].extend(self.category_vectors[old_cat])
        
        # æ›¿æ¢ä¸ºæ–°ç»“æ„
        self.category_vectors = {cat: vecs for cat, vecs in new_vectors.items() if vecs}

    def stage1_summarization(self, sample_size: int = None) -> List[Dict]:
        """
        ç¬¬ä¸€é˜¶æ®µï¼š**ç›´æ¥è¯»å–å·²æœ‰çš„ summary å’Œ vector**ï¼ˆè·³è¿‡æ‘˜è¦ç”Ÿæˆï¼‰
        """
        try:
            if sample_size is None:
                cursor = self.collection.find(
                    {"summary": {"$exists": True, "$ne": "", "$ne": "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"}},
                    no_cursor_timeout=True
                )
                total_count = self.collection.count_documents({
                    "summary": {"$exists": True, "$ne": "", "$ne": "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"}
                })
                print(f"å‡†å¤‡å¤„ç†å…¨éƒ¨ {total_count} ä¸ªå·²æœ‰æ‘˜è¦çš„æ–‡æ¡£...")
            else:
                cursor = self.collection.find(
                    {"summary": {"$exists": True, "$ne": "", "$ne": "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"}},
                    no_cursor_timeout=True
                ).limit(sample_size)
                total_count = min(sample_size, self.collection.count_documents({
                    "summary": {"$exists": True, "$ne": "", "$ne": "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"}
                }))
                print(f"å‡†å¤‡å¤„ç†æœ€å¤š {total_count} ä¸ªå·²æœ‰æ‘˜è¦çš„æ–‡æ¡£...")
        except Exception as e:
            print(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
            return []

        results = []
        processed = 0

        try:
            for doc in cursor:
                try:
                    summary = doc.get("summary", "").strip()
                    vector = doc.get('vector')
                    
                    if not summary or summary in ["å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"]:
                        print(f"æ–‡æ¡£ {doc.get('_id', 'æœªçŸ¥')} æ‘˜è¦æ— æ•ˆï¼Œè·³è¿‡")
                        continue

                    if vector is None:
                        print(f"æ–‡æ¡£ {doc.get('_id', 'æœªçŸ¥')} ç¼ºå°‘ 'vector'ï¼Œè·³è¿‡")
                        continue
                    
                    if isinstance(vector, list):
                        vector = np.array(vector).flatten()
                    else:
                        print(f"æ–‡æ¡£ {doc.get('_id', 'æœªçŸ¥')} å‘é‡æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡")
                        continue

                    result = {
                        "doc_id": str(doc['_id']),
                        "summary": summary,
                        "vector": vector,
                        "processed_at": datetime.now()
                    }
                    
                    results.append(result)
                    self.all_summaries.append(summary)
                    self.document_vectors[str(doc['_id'])] = vector
                    
                    processed += 1
                    if processed % 10 == 0:
                        print(f"å·²åŠ è½½ {processed}/{total_count} ä¸ªæ–‡æ¡£")

                    if (processed) % self.config["batch_size"] == 0:
                        time.sleep(self.config["request_delay"])

                except Exception as e:
                    print(f"å¤„ç†æ–‡æ¡£ {doc.get('_id', 'æœªçŸ¥')} æ—¶å‡ºé”™: {e}")
                    continue

        finally:
            cursor.close()
        
        print(f"ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œå…±æˆåŠŸåŠ è½½ {len(results)} ä¸ªæ–‡æ¡£çš„æ‘˜è¦ä¸å‘é‡")
        return results

    def calculate_category_centroids(self):
        print("å¼€å§‹è®¡ç®—ç±»åˆ«è´¨å¿ƒ...")
        self.category_centroids.clear()
        for category, vectors in self.category_vectors.items():
            if len(vectors) >= self.config["min_samples_for_centroid"]:
                centroid = np.mean(np.array(vectors), axis=0)
                self.category_centroids[category] = centroid
                print(f"   '{category}' â†’ {len(vectors)} æ ·æœ¬ â†’ è´¨å¿ƒç»´åº¦ {len(centroid)}")
            else:
                print(f"   '{category}' æ ·æœ¬ä¸è¶³ ({len(vectors)} < {self.config['min_samples_for_centroid']})ï¼Œè·³è¿‡è´¨å¿ƒè®¡ç®—")

    def find_best_category_by_distance(self, vector: np.ndarray) -> Tuple[str, float]:
        if not self.category_centroids:
            return "å…¶ä»–", float('inf')
        
        min_distance = float('inf')
        best_category = "å…¶ä»–"
        
        for category, centroid in self.category_centroids.items():
            try:
                dist = cosine(vector, centroid)
                if dist < min_distance:
                    min_distance = dist
                    best_category = category
            except Exception as e:
                print(f"è®¡ç®—ä¸ '{category}' è·ç¦»å‡ºé”™: {e}")
        
        return best_category, min_distance

    def assess_potential_new_category(self, text: str, vector: np.ndarray) -> Dict[str, Any]:
        try:
            existing_cats = list(self.category_centroids.keys())
            prompt = f"""
                You are a senior dark web threat intelligence analyst.

                # Background
                - You are reviewing a new dark web text snippet.
                - You have a predefined set of dark web categories (representing distinct domains of dark web content).
                - Your task is to determine whether this text belongs to an existing category or represents a genuinely new category.

                # Rules
                - **Do not alter existing categories**: Only mark as a match if the text **clearly belongs** to one of the existing dark web categories.
                - **Do not force a classification**: If the text describes a distinct threat domain not covered by current categories, propose a new one.
                - **Any new category must**:
                  - Be a **2â€“4 word name phrase**
                  - Represent a **broad but concrete dark web domain** (e.g., "Illicit Firearms Trade")
                  - Use **professional, legally compliant terminology** (e.g., "Child Sexual Abuse Material", not "Child Porn")
                  - Avoid **generic terms**: "Services", "Content", "Platform", "Activities", "Other", "Miscellaneous"
                  - Be **specific enough to guide classification**, yet **not overly narrow** (e.g., use "Payment Card Fraud" instead of "Stolen Visa Cards")

                # Existing Top-Level Categories
                {json.dumps(existing_cats, ensure_ascii=False)}

                # New Text
                {text}

                # Output
                Output ONLY a valid JSON object containing:
                - "fits_existing": true if the text clearly belongs to an existing category, false otherwise.
                - "suggested_category": if "fits_existing" is true, provide the **exact name** of the matching existing category; if false, provide a new top-level category name (2â€“4 words).
                - "reasoning": a brief one-sentence justification.

                Do not include any other text, markdown, or formatting.
                """
            time.sleep(self.config["request_delay"])
            response = self.openai_client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "system", "content": "You are a topic classification expert."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"æ–°ç±»åˆ«è¯„ä¼°å¤±è´¥: {e}")
            return {"fits_existing": True, "suggested_category": "", "reasoning": "è¯„ä¼°å‡ºé”™"}

    def _handle_new_category_addition(self, new_category: str, representative_vector: np.ndarray, taxonomy: List[Dict]):
        new_category = new_category.strip()
        existing_cats = {item["category"] for item in taxonomy}
        if not new_category or new_category in existing_cats:
            return
        new_item = {"category": new_category}
        taxonomy.append(new_item)
        self.category_vectors[new_category] = [representative_vector]
        print(f"æˆåŠŸæ·»åŠ æ–°ç±»åˆ«: '{new_category}'")

    def generate_initial_taxonomy(self, batch_summaries: List[str]) -> List[Dict]:
        try:
            prompt = f"""
            # Instruction
            Generate a taxonomy from dark web content summaries for classifying dark web texts. The taxonomy must be accurate, mutually exclusive, and comprehensive.

            # Context
            You are a senior dark web intelligence analyst. Your task is to derive a concise set of dark web content categories from a batch of textual summaries. These summaries describe the content found on dark web pages.

            # Requirements
            - Each category must be a **2â€“4 word name phrase** representing a **broad but concrete** thematic domain.
            - Categories must be **mutually exclusive**â€”no overlap or contradiction.
            - Output must be in **English only**.
            - Strike the right granularity:
              â€¢ GOOD examples: "Payment Card Fraud", "Illicit Drug Trade", "Malware Distribution"
              â€¢ TOO BROAD: "Illegal Activity", "Cybercrime", "Dark Web"
              â€¢ TOO NARROW: "Stolen Visa Cards", "Forum User Guide", "Marketplace Reviews", "Error 404 Page"
            - STRICTLY AVOID:
              - Generic filler terms: "Services", "Content", "Information", "Platform", "Activities", "Other", "Miscellaneous"
              - Specific items (e.g., "Passports", "Cocaine", "Ransomware") â€” instead, abstract to their **domain**: "Identity Fraud", "Illicit Drug Trade", "Malware & Exploits"
            - The resulting taxonomy should **closely reflect the input data**: do not omit important categories or introduce irrelevant ones.
            - Categories must be **specific, meaningful**, and grounded in the dataâ€”do not invent categories not supported by the summaries.
            - The taxonomy should effectively serve the purpose of dark web content classification.

            # Data
            {json.dumps(batch_summaries, indent=2)}

            # Output
            Output ONLY valid JSON with a top-level key "taxonomy" containing a list of objects in the form {{"category": "..."}}.  
            Do not include any other text, explanations, or formatting.
            """
            time.sleep(self.config["request_delay"])
            response = self.openai_client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "system", "content": "You are a precision-focused dark web taxonomy expert."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            result = json.loads(response.choices[0].message.content)
            taxonomy = result.get("taxonomy", [])
            clean_taxonomy = [{"category": item["category"]} for item in taxonomy if "category" in item]
            print(f"Initial taxonomy generated: {len(clean_taxonomy)} top-level categories")
            return clean_taxonomy
        except Exception as e:
            print(f"Failed to generate initial taxonomy: {e}")
            return []

    def update_taxonomy(self, current_taxonomy: List[Dict], batch_summaries: List[str]) -> Tuple[List[Dict], Dict[str, List[str]]]:
        try:
            current_cats = [item["category"] for item in current_taxonomy]
            prompt = f"""
            You are a senior dark web intelligence analyst updating a flat threat taxonomy (top-level categories only).
            
            # Background
            - You are given a current taxonomy and a batch of new dark web content summaries.
            - Your task is to:  
              (1) evaluate the quality of the current taxonomy against the new data. 
              (2) rate it on a scale from 0 to 100,  
              (3) propose modifications if necessary, and  
              (4) output an improved flat taxonomy.
            
            # Rules
            - **Granularity**: Each category must be a 2â€“4 word phrase representing a broad but concrete criminal ecosystem.  
              â€¢ GOOD examples: "Payment Card Fraud", "Illicit Drug Trade"  
              â€¢ TOO BROAD: "Cybercrime", "Illegal Activity"  
              â€¢ TOO NARROW: "Stolen Visa Cards", "Forum Rules"
            - **Merge similar categories** (e.g., "Fake IDs" + "Stolen SSNs" â†’ "Identity Fraud").
            - **Add new categories only if** the new summaries reveal a clear, recurring threat domain not covered by the current taxonomy.
            - **Remove** categories that are vague (e.g., "Other", "Services"), redundant, or unsupported by the data.
            - **Never use**: "Other", "Miscellaneous", "Services", "Content", "Platform", "Dark Web", "General", "Undefined".
            - **Output must be a flat list** â€” no hierarchy, no subcategories.
            
            # CRITICAL: Output Mapping
            For every category in your updated taxonomy, explicitly list **all categories from the CURRENT taxonomy that were merged into it**.
            - If a category is unchanged, map it to itself.
            - This enables correct migration of historical document vectors.

            # Evaluation Criteria
            ## Intrinsic Quality
            - Are category names clear, consistent, and mutually exclusive?
            - Are they relevant to cybersecurity threat intelligence?
            - Do they contain vague or prohibited terms?
            
            ## Extrinsic Quality
            - Can the taxonomy accurately and unambiguously classify the new summaries?
            - Are there missing threat domains in the current taxonomy?
            - Are there redundant or data-unsupported categories?
            
            # Current Taxonomy
            {json.dumps(current_cats, indent=2, ensure_ascii=False)}
            
            # New Summaries
            {json.dumps(batch_summaries, indent=2, ensure_ascii=False)}
            
            # Output Instructions
            Output ONLY a valid JSON object with the following keys:
            - "rating": an integer from 0 to 100 (higher = better quality)
            - "explanation": a string (â‰¤ 50 words) explaining the rating
            - "suggestion": a string (â‰¤ 30 words) describing necessary edits
            - "updated_taxonomy": a list of objects in the form {{"category": "Consolidated Category Name"}}
            - "merged_from": {{
                  "New Cat A": ["Old Cat 1", "Old Cat 2"],
                  "Unchanged Cat": ["Unchanged Cat"],
                  ...
              }}
            - Keys in "merged_from" must be from "updated_taxonomy".
            - Values must be subsets of the input "Current Taxonomy".
            - Do not include any other text, markdown, or formatting.
            """
            time.sleep(self.config["request_delay"])
            response = self.openai_client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "system", "content": "You are a cyber threat intelligence taxonomy optimizer."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            result = json.loads(response.choices[0].message.content)
            updated = result.get("updated_taxonomy", current_taxonomy)
            merged_from = result.get("merged_from", {})
            
            clean_updated = [{"category": item["category"]} for item in updated if "category" in item]
            clean_merged = {}
            for new_cat, old_list in merged_from.items():
                if isinstance(old_list, list):
                    clean_merged[new_cat] = [str(x).strip() for x in old_list if x and isinstance(x, str)]
            
            print(f"Taxonomy updated: {len(clean_updated)} top-level categories")
            return clean_updated, clean_merged
        except Exception as e:
            print(f"Taxonomy update failed: {e}")
            # å›é€€ï¼šæ— å˜æ›´ï¼Œè‡ªæ˜ å°„
            fallback_merged = {item["category"]: [item["category"]] for item in current_taxonomy}
            return current_taxonomy, fallback_merged

    def final_review_taxonomy(self, taxonomy: List[Dict]) -> Tuple[List[Dict], Dict[str, List[str]]]:
        try:
            cat_names = [item["category"] for item in taxonomy]
            prompt = f"""
            # Task Instruction
            You are a senior dark web intelligence analyst responsible for performing the final review of a **dark web taxonomy**.

            # Background Requirements
            - The output will be used in an **automated dark web classification system**.
            - The taxonomy must adhere to **industry-standard terminology**, such as: "Pornographic Content", "Illegal Marketplaces", "Hacking Services", etc.

            # Rules
            - Each category must be a **2â€“4 word name phrase** representing a **broad but concrete** thematic domain.
            - Categories must be **mutually exclusive**â€”no overlap or contradiction.
            - Output must be in **English only**.
            - Strike the right granularity:
              â€¢ GOOD examples: "Payment Card Fraud", "Illicit Drug Trade", "Malware Distribution"
              â€¢ TOO BROAD: "Illegal Activity", "Cybercrime", "Dark Web"
              â€¢ TOO NARROW: "Stolen Visa Cards", "Forum User Guide", "Marketplace Reviews", "Error 404 Page"
            - STRICTLY AVOID:
              - Generic filler terms: "Services", "Content", "Information", "Platform", "Activities", "Other", "Miscellaneous"
              - Specific items (e.g., "Passports", "Cocaine", "Ransomware") â€” instead, abstract to their **domain**: "Identity Fraud", "Illicit Drug Trade", "Malware & Exploits"
            - The resulting taxonomy should **effectively serve dark web content classification**.
            - Categories must be **specific, meaningful**, and grounded in real-world threat intelligence.

            # Current Categories
            {json.dumps(cat_names, indent=2, ensure_ascii=False)}

            # Output
            Output ONLY a valid JSON object with the following structure:
            {{
                "final_taxonomy": [
                    {{"category": "Consolidated Category Name"}},
                    ...
                ],
                "merged_from": {{
                    "Consolidated Category Name": ["Original Category A", "Original Category B", ...],
                    ...
                }}
            }}
            - Every category in "final_taxonomy" must appear as a key in "merged_from".
            - All values in "merged_from" must be subsets of the input categories.
            - Do not include any other text, markdown, or formatting.
            """
            time.sleep(self.config["request_delay"])
            response = self.openai_client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "system", "content": "You are a precision-focused cyber intelligence taxonomy auditor."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            result = json.loads(response.choices[0].message.content)
            final = result.get("final_taxonomy", taxonomy)
            merged_from = result.get("merged_from", {})
            
            clean_final = [{"category": item["category"]} for item in final if "category" in item]
            clean_merged = {}
            for new_cat, old_list in merged_from.items():
                if isinstance(old_list, list):
                    clean_merged[new_cat] = [str(x).strip() for x in old_list if x and isinstance(x, str)]
            
            print(f"Final taxonomy: {len(clean_final)} top-level categories")
            return clean_final, clean_merged
        except Exception as e:
            print(f"Final review failed: {e}")
            # å›é€€ï¼šè‡ªæ˜ å°„
            fallback_merged = {item["category"]: [item["category"]] for item in taxonomy}
            return taxonomy, fallback_merged

    def find_best_topic_match(self, text: str, topics: List[str]) -> str:
        if not topics:
            return "å…¶ä»–"
        try:
            prompt = f"""
                You are a senior dark web threat intelligence analyst.

                Task: Assign the input text to the SINGLE most appropriate category from the provided list.  
                - ONLY use an **exact name** from the "Categories" list below.  
                - Return **"Other"** ONLY if the text clearly does NOT belong to ANY of the listed categories (e.g., neutral content like "Bible Study", "Error 404", "Privacy Guide").  
                - NEVER paraphrase, merge, or invent categories.  
                - If the text relates to illegal or illicit activity, it almost certainly fits one of the threat categories â€” do NOT default to "Other".

                Text: {text}

                Categories: {', '.join(topics)}

                Output ONLY the category name or "Other". No explanation, no punctuation, no extra text.
                """
            time.sleep(self.config["request_delay"])
            response = self.openai_client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "system", "content": "You are a precise classifier. Output ONLY the exact category name or 'Others'."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0
            )
            result = response.choices[0].message.content.strip().strip('"').strip('"')
            return result if result in topics else "å…¶ä»–"
        except Exception as e:
            print(f"ä¸»é¢˜åŒ¹é…å¤±è´¥ï¼Œå›é€€åˆ°'å…¶ä»–': {e}")
            return "å…¶ä»–"

    def assign_categories_to_documents(self, documents: List[Dict], taxonomy: List[Dict]) -> List[Dict]:
        level1_cats = [item["category"] for item in taxonomy]
        for doc in documents:
            summary = doc["summary"]
            cat = self.find_best_topic_match(summary, level1_cats)
            doc["category"] = cat
        return documents

    def stage2_taxonomy_building(self, documents: List[Dict]) -> List[Dict]:
        if not documents:
            return []

        batch_size = self.config["taxonomy_batch_size"]
        batches = [documents[i:i+batch_size] for i in range(0, len(documents), batch_size)]
        print(f"å¼€å§‹æ„å»ºä¸€çº§åˆ†ç±»æ³•ï¼ˆå…± {len(batches)} æ‰¹ï¼‰...")

        taxonomy = []

        for idx, batch in enumerate(batches):
            summaries = [d["summary"] for d in batch]
            if idx == 0:
                taxonomy = self.generate_initial_taxonomy(summaries)
            else:
                taxonomy, merged_from = self.update_taxonomy(taxonomy, summaries)
                self._migrate_category_vectors(merged_from)

            # ğŸ‘‡ ç»Ÿä¸€åˆå§‹åŒ–ï¼šç¡®ä¿ taxonomy ä¸­æ‰€æœ‰ç±»åˆ«éƒ½åœ¨ category_vectors ä¸­
            for item in taxonomy:
                cat = item["category"]
                if cat not in self.category_vectors:
                    self.category_vectors[cat] = []
            
            self.assign_categories_to_documents(batch, taxonomy)

            for doc in batch:
                cat = doc.get("category")
                vec = doc.get("vector")
                if cat and cat != "å…¶ä»–" and vec is not None:
                    self.category_vectors[cat].append(vec)

            self.calculate_category_centroids()

            for doc in batch:
                cat = doc.get("category")
                vec = doc.get("vector")
                if cat == "å…¶ä»–" or vec is None:
                    continue
                centroid = self.category_centroids.get(cat)
                if centroid is None:
                    continue
                dist = cosine(vec, centroid)
                if dist > self.config["distance_threshold"]:
                    print(f"æ–‡æ¡£ {doc['doc_id']} è·ç¦»è´¨å¿ƒè¿‡å¤§ ({dist:.3f})ï¼Œè¯„ä¼°æ˜¯å¦éœ€æ–°ç±»åˆ«...")
                    llm_res = self.assess_potential_new_category(doc["summary"], vec)
                    if not llm_res.get("fits_existing", True):
                        new_cat = llm_res.get("suggested_category", "").strip()
                        if new_cat:
                            self._handle_new_category_addition(new_cat, vec, taxonomy)
                            self.calculate_category_centroids()

            print(f"æ‰¹æ¬¡ {idx+1}/{len(batches)} å¤„ç†å®Œæˆ")

        final_taxonomy, merged_from = self.final_review_taxonomy(taxonomy)
        self._migrate_category_vectors(merged_from)
        self.final_taxonomy = [{"category": item["category"]} for item in final_taxonomy if "category" in item]
        return self.final_taxonomy

    def export_to_json(self, documents: List[Dict], output_path: str = "output/full_taxonomy_results.json"):
        try:
            output_data = []
            for doc in documents:
                summary = doc.get("summary", "").strip()
                if not summary or summary == "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦":
                    continue
                category = doc.get("category", "æœªåˆ†ç±»").strip()
                output_data.append({
                    "alert": summary,
                    "category": category
                })
            
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"åˆ†ç±»ç»“æœå·²å¯¼å‡ºä¸º JSON æ–‡ä»¶: {output_path}")
        except Exception as e:
            print(f"JSON å¯¼å‡ºå¤±è´¥: {e}")

    def taxonomy_to_cluster_table(self, taxonomy, max_num_clusters=None):
        # Extract category names
        categories = [item["category"] for item in taxonomy if "category" in item]
        
        # Apply max_num_clusters if specified
        if max_num_clusters is not None:
            categories = categories[:max_num_clusters]
        
        # Build markdown table lines
        lines = [
            "|id|name|description|",
            "|-|-|-|"
        ]
        
        for idx, name in enumerate(categories, start=1):
            # Escape any pipe characters in name to avoid breaking markdown table
            safe_name = str(name).replace("|", "\\|")
            lines.append(f"|{idx}|{safe_name}| |")
        
        return "\n".join(lines)
    
    def phase2_classification_sample(self, alerts_to_classify_sample, cluster_table):
        """
        æ‰§è¡Œ TnT-LLM çš„ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ç”Ÿæˆçš„åˆ†ç±»æ³•å¯¹ä¸€ä¸ªæ ·æœ¬è¿›è¡Œ LLM æ ‡æ³¨ï¼ˆç”Ÿæˆä¼ªæ ‡ç­¾ï¼‰ã€‚
        """
        print("- Starting Phase 2: LLM Annotation for Sample (Pseudo-labeling) -")
        labeled_results = []
        explanation_length = 100
        
        for i, alert in enumerate(alerts_to_classify_sample):
            # ä½¿ç”¨ Prompt è¿›è¡Œåˆ†ç±»
            prompt = """
            # Instruction
            ## Context
            - **Goal**: Your goal is to classify the input data using the provided reference table.
            - **Reference table**: The input reference table is a markdown table with each row as a category, with the following columns:
                - **id**: category index.
                - **name**: category name.
                - **description**: category description used to classify data points.
            - **Data**: Your input data is a conversation history between a User and an AI agent.

            # Reference table
            {cluster_table}

            # Data
            {input_text}

            # Questions
            ## Please classify the input data using the reference table. Your output should include the following information:
            - **category-id**: **id** of a category in the reference table; if unable to classify using the reference table, please output "-1".
            - **category-name**: **name** of a category in the reference table that corresponds to the **category-id**; if unable to classify using the reference table, please output "Undefined".
            - **explanation**: a short explanation of why you think the input data belongs to the category or you cannot classify the data into any of the given categories. You explanation should be within {explanation_length} words.
            Tips
            - You should only output the **primary** category for the input data. If it can be classified into multiple categories, please output **the most relevant category**.
            - Your output should be in *English* only.

            ## Please provide your answers between the tags: <category-id>your identified category id</category-id>, <category-name>your identified category name</category-name>, <explanation>your explanation</explanation>.

            # Output
            """
            prompt = prompt.format(cluster_table=cluster_table, input_text=alert, explanation_length=explanation_length)
            
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-5-mini",
                    messages=[
                        {"role": "system", "content": "You are a precise classifier."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.6
                )
                classification_response = response.choices[0].message.content.strip().strip('"').strip('"')
            except Exception as e:
                print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                classification_response = None

            if classification_response:
                try:
                    # è§£æ LLM çš„è¾“å‡ºï¼Œæå– <category-id>, <category-name>, <explanation>
                    cat_id_start = "<category-id>"
                    cat_id_end = "</category-id>"
                    cat_name_start = "<category-name>"
                    cat_name_end = "</category-name>"
                    exp_start = "<explanation>"
                    exp_end = "</explanation>"

                    cat_id = classification_response.split(cat_id_start)[1].split(cat_id_end)[0].strip()
                    cat_name = classification_response.split(cat_name_start)[1].split(cat_name_end)[0].strip()
                    explanation = classification_response.split(exp_start)[1].split(exp_end)[0].strip()

                    labeled_results.append({
                        "original_alert": alert,
                        "category_id": cat_id,
                        "category_name": cat_name,
                        "explanation": explanation
                    })
                    print(f" LLM Annotated sample {i+1}/{len(alerts_to_classify_sample)}")
                except Exception as e:
                    print(f" Error parsing classification for sample {i+1}: {e}")
                    labeled_results.append({
                        "original_alert": alert,
                        "category_id": "-1",
                        "category_name": "Undefined",
                        "explanation": "Failed to parse LLM response."
                    })
            else:
                print(f" Failed to classify sample {i+1}")
                labeled_results.append({
                    "original_alert": alert,
                    "category_id": "-1",
                    "category_name": "Undefined",
                    "explanation": "LLM call failed."
                })

        print("- Phase 2 (Sample Annotation) Completed -")
        return labeled_results
    
    def train_lightweight_classifier_with_precomputed_vectors(self, labeled_data, precomputed_vectors, model_type='logistic_regression', model_save_path='lightweight_model_with_vectors.pkl'):
        """
        ä½¿ç”¨é¢„è®¡ç®—çš„å‘é‡è®­ç»ƒä¸€ä¸ªè½»é‡çº§åˆ†ç±»å™¨ã€‚
        """
        print(f"- Training Lightweight Classifier with Precomputed Vectors ({model_type}) -")
        
        if len(labeled_data) != len(precomputed_vectors):
            raise ValueError(f"Labeled data length ({len(labeled_data)}) doesn't match vectors length ({len(precomputed_vectors)})")
        
        labels = [item['category_name'] for item in labeled_data]
        
        # ä½¿ç”¨é¢„è®¡ç®—çš„å‘é‡ä½œä¸ºç‰¹å¾
        X = np.array(precomputed_vectors)
        
        # ç¡®ä¿Xæ˜¯2Dæ•°ç»„
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        elif X.ndim == 3:
            X = X.reshape(X.shape[0], -1)
        
        print(f"Training data shape: X={X.shape}, y={len(labels)}")

        # æ ¹æ®æŒ‡å®šç±»å‹åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        if model_type == 'logistic_regression':
            model = LogisticRegression(random_state=42, max_iter=1000)
            model.fit(X, labels)
        elif model_type == 'mlp':
            from sklearn.neural_network import MLPClassifier
            model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
            model.fit(X, labels)
        elif model_type == 'lightgbm':
            import lightgbm as lgb
            from sklearn.preprocessing import LabelEncoder
            
            # LightGBM éœ€è¦å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
            self.label_encoder = LabelEncoder()
            y_encoded = self.label_encoder.fit_transform(labels)
            
            # ä½¿ç”¨LightGBM sklearn API
            model = lgb.LGBMClassifier(random_state=42)
            model.fit(X, y_encoded)
            
            # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
            label_encoder_save_path = model_save_path.replace('.pkl', '_label_encoder.pkl')
            with open(label_encoder_save_path, 'wb') as f:
                pickle.dump(self.label_encoder, f)
            print(f"Label encoder saved to {label_encoder_save_path}")
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

        # ä¿å­˜æ¨¡å‹
        with open(model_save_path, 'wb') as f:
            pickle.dump(model, f)

        print(f"Model saved to {model_save_path}")
        print("- Lightweight Classifier Training with Precomputed Vectors Completed -")
        return model
    
    def apply_lightweight_classifier_with_vectors(self, precomputed_vectors, model, model_type='logistic_regression', label_encoder_path=None):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„è½»é‡çº§åˆ†ç±»å™¨å’Œé¢„è®¡ç®—çš„å‘é‡å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚
        """
        print("- Applying Lightweight Classifier with Precomputed Vectors -")
        
        # ä½¿ç”¨é¢„è®¡ç®—çš„å‘é‡ä½œä¸ºç‰¹å¾
        X = np.array(precomputed_vectors)
        
        # ç¡®ä¿Xæ˜¯2Dæ•°ç»„
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        elif X.ndim == 3:
            X = X.reshape(X.shape[0], -1)
        
        print(f"Prediction data shape: X={X.shape}")

        predictions = model.predict(X)

        # å¦‚æœæ˜¯ LightGBMï¼Œéœ€è¦å°†é¢„æµ‹çš„ç¼–ç è½¬å›åŸå§‹æ ‡ç­¾å
        if model_type == 'lightgbm' and label_encoder_path and os.path.exists(label_encoder_path):
            with open(label_encoder_path, 'rb') as f:
                label_encoder = pickle.load(f)
            predictions = label_encoder.inverse_transform(predictions.astype(int))
        elif model_type == 'lightgbm' and not (label_encoder_path and os.path.exists(label_encoder_path)):
            print("Warning: Model type is LightGBM but label encoder path not provided or found. Predictions might be encoded integers.")

        results = []
        for i, pred_label in enumerate(predictions):
            results.append({
                "predicted_category": pred_label,
                "explanation": "Generated by lightweight classifier with precomputed vectors"
            })
        print("- Lightweight Classification with Precomputed Vectors Completed -")
        return results

    def classify_all_documents_ml(self, taxonomy: List[Dict]):
        """
        ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆé€»è¾‘å›å½’ï¼‰å¯¹å…¨é‡æ–‡æ¡£è¿›è¡Œåˆ†ç±»
        """
        print("=== å¼€å§‹ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹å…¨é‡æ–‡æ¡£è¿›è¡Œåˆ†ç±» ===")
        
        # 1. è·å–æ‰€æœ‰å«æœ‰æ•ˆsummaryçš„æ–‡æ¡£
        try:
            cursor = self.collection.find(
                {"summary": {"$exists": True, "$ne": "", "$ne": "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"}},
                {"_id": 1, "summary": 1, "vector": 1}
            )
            docs_list = list(cursor)
            total = len(docs_list)
            print(f"å…±æ‰¾åˆ° {total} ä¸ªå«æœ‰æ•ˆ summary çš„æ–‡æ¡£")
        except Exception as e:
            print(f"æŸ¥è¯¢å…¨é‡æ–‡æ¡£å¤±è´¥: {e}")
            return

        if not docs_list:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£è¿›è¡Œåˆ†ç±»")
            return

        # æå–æ–‡æ¡£å†…å®¹ã€å‘é‡å’ŒID
        summaries = [doc.get("summary", "") for doc in docs_list]
        vectors = []
        doc_ids = []
        
        for doc in docs_list:
            vector = doc.get("vector")
            if vector is not None:
                if isinstance(vector, list):
                    vector = np.array(vector).flatten()
                vectors.append(vector)
                doc_ids.append(doc["_id"])
            else:
                # å¦‚æœæ–‡æ¡£æ²¡æœ‰å‘é‡ï¼Œæš‚æ—¶è·³è¿‡
                vectors.append(None)
                doc_ids.append(doc["_id"])

        # è¿‡æ»¤æ‰æ²¡æœ‰å‘é‡çš„æ–‡æ¡£
        valid_indices = [i for i, v in enumerate(vectors) if v is not None]
        valid_summaries = [summaries[i] for i in valid_indices]
        valid_vectors = [vectors[i] for i in valid_indices]
        valid_doc_ids = [doc_ids[i] for i in valid_indices]

        if not valid_vectors:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å‘é‡è¿›è¡Œåˆ†ç±»")
            return

        print(f"å…±æœ‰ {len(valid_vectors)} ä¸ªæ–‡æ¡£å…·æœ‰æœ‰æ•ˆå‘é‡ï¼Œå‡†å¤‡ä½¿ç”¨LLMç”Ÿæˆå°æ•°æ®é›†å¹¶è®­ç»ƒåˆ†ç±»å™¨...")

        # 2. ä»æœ‰æ•ˆæ–‡æ¡£ä¸­é‡‡æ ·ä¸€éƒ¨åˆ†ç”¨äºLLMæ ‡æ³¨
        sample_size = min(100, len(valid_summaries))  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ ·æœ¬å¤§å°
        sampled_summaries = valid_summaries[:sample_size]
        sampled_vectors = valid_vectors[:sample_size]
        sampled_doc_ids = valid_doc_ids[:sample_size]

        print(f"ä½¿ç”¨ {sample_size} ä¸ªæ–‡æ¡£æ ·æœ¬è¯·æ±‚LLMç”Ÿæˆæ ‡æ³¨æ•°æ®...")

        # 3. ä½¿ç”¨ LLM å¯¹æ ·æœ¬è¿›è¡Œæ ‡æ³¨
        cluster_table = self.taxonomy_to_cluster_table(taxonomy)
        pseudo_labeled_data = self.phase2_classification_sample(sampled_summaries, cluster_table)

        # 4. è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨
        model_type_choice = 'logistic_regression'
        model_save_path = f'lightML_{model_type_choice}_model_with_vectors.pkl'
        
        self.ml_model = self.train_lightweight_classifier_with_precomputed_vectors(
            labeled_data=pseudo_labeled_data,
            precomputed_vectors=sampled_vectors[:len(pseudo_labeled_data)],  # åªä½¿ç”¨æœ‰æ ‡ç­¾çš„æ•°æ®å¯¹åº”çš„å‘é‡
            model_type=model_type_choice,
            model_save_path=model_save_path
        )

        # 5. åº”ç”¨é€»è¾‘å›å½’åˆ†ç±»å™¨å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œåˆ†ç±»
        print(f"åº”ç”¨é€»è¾‘å›å½’åˆ†ç±»å™¨å¯¹ {len(valid_summaries)} ä¸ªæ–‡æ¡£è¿›è¡Œåˆ†ç±»...")
        final_results = self.apply_lightweight_classifier_with_vectors(
            precomputed_vectors=valid_vectors,
            model=self.ml_model,
            model_type=model_type_choice
        )
        print(f"åˆ†ç±»å®Œæˆï¼Œå…±å¤„ç† {len(final_results)} ä¸ªæ–‡æ¡£")

        # 6. æ›´æ–°æ•°æ®åº“ä¸­çš„åˆ†ç±»ç»“æœ
        batch_for_update = []
        for i, result in enumerate(final_results):
            if i < len(valid_doc_ids):
                batch_for_update.append(
                    pymongo.UpdateOne(
                        {"_id": valid_doc_ids[i]},
                        {
                            "$set": {
                                "category": result['predicted_category'],
                                "classification_updated_at": datetime.now()
                            }
                        }
                    )
                )
            
            # æ‰¹é‡æ›´æ–°æ•°æ®åº“
            if len(batch_for_update) >= 100:
                try:
                    self.collection.bulk_write(batch_for_update, ordered=False)
                    batch_for_update = []
                except Exception as e:
                    print(f"æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")

        # å¤„ç†å‰©ä½™æ›´æ–°
        if batch_for_update:
            try:
                self.collection.bulk_write(batch_for_update, ordered=False)
            except Exception as e:
                print(f"æœ€åä¸€æ‰¹æ›´æ–°å¤±è´¥: {e}")

        # 7. å¯¼å‡ºç»“æœåˆ°JSONæ–‡ä»¶
        save_results = []
        for i, result in enumerate(final_results):
            if i < len(valid_summaries):
                save_result = {
                    "alert": valid_summaries[i],
                    "category": result['predicted_category']
                }
                save_results.append(save_result)

        # ä¿å­˜å®Œæ•´ç»“æœåˆ°æ–‡ä»¶
        os.makedirs("output", exist_ok=True)
        output_path = "output/full_taxonomy_results_LR.json"
        with open(output_path, "w", encoding='utf-8') as f:
            json.dump(save_results, f, indent=4, ensure_ascii=False)
        print(f"å®Œæ•´åˆ†ç±»ç»“æœå·²ä¿å­˜åˆ° {output_path}")
        
        return save_results

    def classify_all_documents(self, taxonomy: List[Dict]):
        """
        åŒ…è£…å‡½æ•°ï¼Œå¯ä»¥é€‰æ‹©ä½¿ç”¨å“ªç§åˆ†ç±»æ–¹æ³•
        """
        # ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œåˆ†ç±»
        return self.classify_all_documents_ml(taxonomy)

    def run_analysis(self, sample_size_for_taxonomy: int = 500):
        print("=== ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºæŠ½æ ·æ„å»ºä¸€çº§åˆ†ç±»ä½“ç³»ï¼ˆä½¿ç”¨å·²æœ‰ summaryï¼‰===")
        docs_sample = self.stage1_summarization(sample_size=sample_size_for_taxonomy)

        if not docs_sample:
            print("æ— æœ‰æ•ˆæ ·æœ¬ï¼Œæ— æ³•æ„å»º taxonomy")
            return

        print("\n=== ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºä¸€çº§ taxonomy ===")
        taxonomy = self.stage2_taxonomy_building(docs_sample)
        self.final_taxonomy = taxonomy

        print("\n=== ç¬¬ä¸‰é˜¶æ®µï¼šä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹å…¨é‡æ–‡æ¡£è¿›è¡Œä¸€çº§åˆ†ç±» ===")
        classified_docs = self.classify_all_documents(taxonomy)

        print("\nå…¨æµç¨‹å®Œæˆï¼")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_results(classified_docs)

    def _print_results(self, documents: List[Dict]):
        print("\n" + "="*60)
        print("æœ€ç»ˆåˆ†ææŠ¥å‘Š")
        print("="*60)
        print(f"æ€»å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
        print(f"ä¸€çº§ä¸»é¢˜æ•°: {len(self.final_taxonomy)}")

        dist = defaultdict(int)
        for d in documents:
            dist[d.get('category', 'æœªåˆ†ç±»')] += 1
        for topic, cnt in sorted(dist.items(), key=lambda x: -x[1]):
            print(f"  â€¢ {topic}: {cnt} ç¯‡")


def main():
    analyzer = TaxonomyBuilder()
    try:
        analyzer.run_analysis(sample_size_for_taxonomy=5000)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()